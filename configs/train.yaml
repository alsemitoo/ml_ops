preprocess:
  target_height: 128
  max_width: 640
  brightness: 0.2
  contrast: 0.2

model: 
  vocab_size: null
  d_model: 64
  nhead: 2
  num_decoder_layers: 1

training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.001
  data_path: "data/raw/default_train"